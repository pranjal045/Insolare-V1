{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.24640946212334552,
  "eval_steps": 500,
  "global_step": 1750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007040270346381301,
      "grad_norm": 40.89167785644531,
      "learning_rate": 4.999061297287149e-05,
      "loss": 4.8231,
      "step": 5
    },
    {
      "epoch": 0.0014080540692762602,
      "grad_norm": 32.68000411987305,
      "learning_rate": 4.997887918896086e-05,
      "loss": 4.5217,
      "step": 10
    },
    {
      "epoch": 0.00211208110391439,
      "grad_norm": 63.9595832824707,
      "learning_rate": 4.996714540505023e-05,
      "loss": 3.885,
      "step": 15
    },
    {
      "epoch": 0.0028161081385525205,
      "grad_norm": 42.39142990112305,
      "learning_rate": 4.9955411621139583e-05,
      "loss": 4.1911,
      "step": 20
    },
    {
      "epoch": 0.0035201351731906504,
      "grad_norm": 52.38974380493164,
      "learning_rate": 4.994367783722895e-05,
      "loss": 4.4811,
      "step": 25
    },
    {
      "epoch": 0.00422416220782878,
      "grad_norm": 42.666236877441406,
      "learning_rate": 4.993194405331832e-05,
      "loss": 3.6912,
      "step": 30
    },
    {
      "epoch": 0.004928189242466911,
      "grad_norm": 34.63199234008789,
      "learning_rate": 4.992021026940768e-05,
      "loss": 3.4672,
      "step": 35
    },
    {
      "epoch": 0.005632216277105041,
      "grad_norm": 46.77542495727539,
      "learning_rate": 4.9908476485497045e-05,
      "loss": 3.1366,
      "step": 40
    },
    {
      "epoch": 0.006336243311743171,
      "grad_norm": 47.924560546875,
      "learning_rate": 4.989674270158641e-05,
      "loss": 3.5856,
      "step": 45
    },
    {
      "epoch": 0.007040270346381301,
      "grad_norm": 46.4420280456543,
      "learning_rate": 4.9885008917675776e-05,
      "loss": 2.9893,
      "step": 50
    },
    {
      "epoch": 0.007744297381019431,
      "grad_norm": 33.90864562988281,
      "learning_rate": 4.987327513376514e-05,
      "loss": 3.8332,
      "step": 55
    },
    {
      "epoch": 0.00844832441565756,
      "grad_norm": 64.43006896972656,
      "learning_rate": 4.98615413498545e-05,
      "loss": 3.5513,
      "step": 60
    },
    {
      "epoch": 0.009152351450295691,
      "grad_norm": 22.94978141784668,
      "learning_rate": 4.984980756594387e-05,
      "loss": 3.9566,
      "step": 65
    },
    {
      "epoch": 0.009856378484933821,
      "grad_norm": 43.99895477294922,
      "learning_rate": 4.983807378203324e-05,
      "loss": 3.7552,
      "step": 70
    },
    {
      "epoch": 0.010560405519571952,
      "grad_norm": 52.061378479003906,
      "learning_rate": 4.982633999812259e-05,
      "loss": 3.7898,
      "step": 75
    },
    {
      "epoch": 0.011264432554210082,
      "grad_norm": 35.40381622314453,
      "learning_rate": 4.981460621421196e-05,
      "loss": 3.0518,
      "step": 80
    },
    {
      "epoch": 0.011968459588848212,
      "grad_norm": 57.40424346923828,
      "learning_rate": 4.980287243030133e-05,
      "loss": 3.818,
      "step": 85
    },
    {
      "epoch": 0.012672486623486343,
      "grad_norm": 43.04438018798828,
      "learning_rate": 4.979113864639069e-05,
      "loss": 4.1768,
      "step": 90
    },
    {
      "epoch": 0.013376513658124471,
      "grad_norm": 54.796607971191406,
      "learning_rate": 4.9779404862480054e-05,
      "loss": 3.7578,
      "step": 95
    },
    {
      "epoch": 0.014080540692762602,
      "grad_norm": 61.69084548950195,
      "learning_rate": 4.9767671078569416e-05,
      "loss": 3.0274,
      "step": 100
    },
    {
      "epoch": 0.014784567727400732,
      "grad_norm": 38.821285247802734,
      "learning_rate": 4.9755937294658785e-05,
      "loss": 3.7498,
      "step": 105
    },
    {
      "epoch": 0.015488594762038862,
      "grad_norm": 30.885408401489258,
      "learning_rate": 4.974420351074815e-05,
      "loss": 3.5427,
      "step": 110
    },
    {
      "epoch": 0.016192621796676993,
      "grad_norm": 30.939821243286133,
      "learning_rate": 4.973246972683751e-05,
      "loss": 3.4858,
      "step": 115
    },
    {
      "epoch": 0.01689664883131512,
      "grad_norm": 47.12916946411133,
      "learning_rate": 4.972073594292688e-05,
      "loss": 3.5638,
      "step": 120
    },
    {
      "epoch": 0.017600675865953253,
      "grad_norm": 66.01870727539062,
      "learning_rate": 4.9709002159016246e-05,
      "loss": 3.736,
      "step": 125
    },
    {
      "epoch": 0.018304702900591382,
      "grad_norm": 21.96756935119629,
      "learning_rate": 4.96972683751056e-05,
      "loss": 2.927,
      "step": 130
    },
    {
      "epoch": 0.019008729935229514,
      "grad_norm": 33.149871826171875,
      "learning_rate": 4.968553459119497e-05,
      "loss": 3.4809,
      "step": 135
    },
    {
      "epoch": 0.019712756969867642,
      "grad_norm": 24.110876083374023,
      "learning_rate": 4.967380080728434e-05,
      "loss": 4.0281,
      "step": 140
    },
    {
      "epoch": 0.020416784004505775,
      "grad_norm": 25.98638916015625,
      "learning_rate": 4.96620670233737e-05,
      "loss": 4.3386,
      "step": 145
    },
    {
      "epoch": 0.021120811039143903,
      "grad_norm": 28.055200576782227,
      "learning_rate": 4.965033323946306e-05,
      "loss": 3.6602,
      "step": 150
    },
    {
      "epoch": 0.021824838073782032,
      "grad_norm": 33.88705062866211,
      "learning_rate": 4.963859945555243e-05,
      "loss": 3.1766,
      "step": 155
    },
    {
      "epoch": 0.022528865108420164,
      "grad_norm": 38.468475341796875,
      "learning_rate": 4.9626865671641794e-05,
      "loss": 3.6751,
      "step": 160
    },
    {
      "epoch": 0.023232892143058292,
      "grad_norm": 35.559818267822266,
      "learning_rate": 4.9615131887731156e-05,
      "loss": 3.5492,
      "step": 165
    },
    {
      "epoch": 0.023936919177696424,
      "grad_norm": 22.099853515625,
      "learning_rate": 4.960339810382052e-05,
      "loss": 3.7572,
      "step": 170
    },
    {
      "epoch": 0.024640946212334553,
      "grad_norm": 38.26958465576172,
      "learning_rate": 4.959166431990989e-05,
      "loss": 3.4648,
      "step": 175
    },
    {
      "epoch": 0.025344973246972685,
      "grad_norm": 34.8751335144043,
      "learning_rate": 4.9579930535999256e-05,
      "loss": 3.2843,
      "step": 180
    },
    {
      "epoch": 0.026049000281610814,
      "grad_norm": 23.299116134643555,
      "learning_rate": 4.956819675208861e-05,
      "loss": 3.0756,
      "step": 185
    },
    {
      "epoch": 0.026753027316248942,
      "grad_norm": 20.394390106201172,
      "learning_rate": 4.955646296817798e-05,
      "loss": 2.961,
      "step": 190
    },
    {
      "epoch": 0.027457054350887074,
      "grad_norm": 27.4729061126709,
      "learning_rate": 4.954472918426735e-05,
      "loss": 2.5313,
      "step": 195
    },
    {
      "epoch": 0.028161081385525203,
      "grad_norm": 41.75809860229492,
      "learning_rate": 4.953299540035671e-05,
      "loss": 3.4827,
      "step": 200
    },
    {
      "epoch": 0.028865108420163335,
      "grad_norm": 52.89690399169922,
      "learning_rate": 4.952126161644607e-05,
      "loss": 3.6554,
      "step": 205
    },
    {
      "epoch": 0.029569135454801464,
      "grad_norm": 23.42662239074707,
      "learning_rate": 4.950952783253544e-05,
      "loss": 3.866,
      "step": 210
    },
    {
      "epoch": 0.030273162489439596,
      "grad_norm": 45.87882995605469,
      "learning_rate": 4.94977940486248e-05,
      "loss": 4.5058,
      "step": 215
    },
    {
      "epoch": 0.030977189524077724,
      "grad_norm": 34.66089630126953,
      "learning_rate": 4.9486060264714165e-05,
      "loss": 3.2622,
      "step": 220
    },
    {
      "epoch": 0.031681216558715856,
      "grad_norm": 39.22142791748047,
      "learning_rate": 4.9474326480803534e-05,
      "loss": 2.9137,
      "step": 225
    },
    {
      "epoch": 0.032385243593353985,
      "grad_norm": 27.13752555847168,
      "learning_rate": 4.9462592696892896e-05,
      "loss": 3.265,
      "step": 230
    },
    {
      "epoch": 0.033089270627992114,
      "grad_norm": 48.745296478271484,
      "learning_rate": 4.9450858912982265e-05,
      "loss": 3.3546,
      "step": 235
    },
    {
      "epoch": 0.03379329766263024,
      "grad_norm": 84.69158172607422,
      "learning_rate": 4.943912512907162e-05,
      "loss": 4.4283,
      "step": 240
    },
    {
      "epoch": 0.03449732469726838,
      "grad_norm": 34.37588119506836,
      "learning_rate": 4.942739134516099e-05,
      "loss": 3.3098,
      "step": 245
    },
    {
      "epoch": 0.035201351731906506,
      "grad_norm": 25.733043670654297,
      "learning_rate": 4.941565756125036e-05,
      "loss": 3.342,
      "step": 250
    },
    {
      "epoch": 0.035905378766544635,
      "grad_norm": 42.15028381347656,
      "learning_rate": 4.940392377733972e-05,
      "loss": 3.5938,
      "step": 255
    },
    {
      "epoch": 0.036609405801182764,
      "grad_norm": 25.860599517822266,
      "learning_rate": 4.939218999342908e-05,
      "loss": 3.3239,
      "step": 260
    },
    {
      "epoch": 0.03731343283582089,
      "grad_norm": 42.7297477722168,
      "learning_rate": 4.938045620951845e-05,
      "loss": 3.0864,
      "step": 265
    },
    {
      "epoch": 0.03801745987045903,
      "grad_norm": 43.67538833618164,
      "learning_rate": 4.936872242560781e-05,
      "loss": 3.2712,
      "step": 270
    },
    {
      "epoch": 0.038721486905097156,
      "grad_norm": 38.44990158081055,
      "learning_rate": 4.9356988641697174e-05,
      "loss": 3.6924,
      "step": 275
    },
    {
      "epoch": 0.039425513939735285,
      "grad_norm": 40.00017547607422,
      "learning_rate": 4.934525485778654e-05,
      "loss": 3.7598,
      "step": 280
    },
    {
      "epoch": 0.040129540974373414,
      "grad_norm": 36.930076599121094,
      "learning_rate": 4.9333521073875905e-05,
      "loss": 3.262,
      "step": 285
    },
    {
      "epoch": 0.04083356800901155,
      "grad_norm": 33.859981536865234,
      "learning_rate": 4.9321787289965274e-05,
      "loss": 3.2338,
      "step": 290
    },
    {
      "epoch": 0.04153759504364968,
      "grad_norm": 28.2403564453125,
      "learning_rate": 4.9310053506054636e-05,
      "loss": 3.5681,
      "step": 295
    },
    {
      "epoch": 0.042241622078287806,
      "grad_norm": 23.90397834777832,
      "learning_rate": 4.9298319722144e-05,
      "loss": 3.3314,
      "step": 300
    },
    {
      "epoch": 0.042945649112925935,
      "grad_norm": 22.783897399902344,
      "learning_rate": 4.9286585938233367e-05,
      "loss": 3.8312,
      "step": 305
    },
    {
      "epoch": 0.043649676147564063,
      "grad_norm": 34.41892623901367,
      "learning_rate": 4.927485215432273e-05,
      "loss": 2.6223,
      "step": 310
    },
    {
      "epoch": 0.0443537031822022,
      "grad_norm": 36.3243522644043,
      "learning_rate": 4.926311837041209e-05,
      "loss": 3.7449,
      "step": 315
    },
    {
      "epoch": 0.04505773021684033,
      "grad_norm": 17.400802612304688,
      "learning_rate": 4.925138458650146e-05,
      "loss": 3.6635,
      "step": 320
    },
    {
      "epoch": 0.045761757251478456,
      "grad_norm": 27.569210052490234,
      "learning_rate": 4.923965080259082e-05,
      "loss": 2.8905,
      "step": 325
    },
    {
      "epoch": 0.046465784286116585,
      "grad_norm": 29.448806762695312,
      "learning_rate": 4.922791701868018e-05,
      "loss": 3.6434,
      "step": 330
    },
    {
      "epoch": 0.04716981132075472,
      "grad_norm": 22.272096633911133,
      "learning_rate": 4.921618323476955e-05,
      "loss": 3.6286,
      "step": 335
    },
    {
      "epoch": 0.04787383835539285,
      "grad_norm": 36.56483840942383,
      "learning_rate": 4.9204449450858914e-05,
      "loss": 2.7799,
      "step": 340
    },
    {
      "epoch": 0.04857786539003098,
      "grad_norm": 26.0479793548584,
      "learning_rate": 4.919271566694828e-05,
      "loss": 2.8886,
      "step": 345
    },
    {
      "epoch": 0.049281892424669106,
      "grad_norm": 25.9864501953125,
      "learning_rate": 4.9180981883037645e-05,
      "loss": 3.4073,
      "step": 350
    },
    {
      "epoch": 0.049985919459307235,
      "grad_norm": 23.160335540771484,
      "learning_rate": 4.916924809912701e-05,
      "loss": 2.7087,
      "step": 355
    },
    {
      "epoch": 0.05068994649394537,
      "grad_norm": 29.77461814880371,
      "learning_rate": 4.9157514315216376e-05,
      "loss": 3.8988,
      "step": 360
    },
    {
      "epoch": 0.0513939735285835,
      "grad_norm": 32.09275817871094,
      "learning_rate": 4.914578053130574e-05,
      "loss": 3.4596,
      "step": 365
    },
    {
      "epoch": 0.05209800056322163,
      "grad_norm": 34.322452545166016,
      "learning_rate": 4.91340467473951e-05,
      "loss": 3.9829,
      "step": 370
    },
    {
      "epoch": 0.052802027597859756,
      "grad_norm": 38.98609924316406,
      "learning_rate": 4.912231296348447e-05,
      "loss": 3.1177,
      "step": 375
    },
    {
      "epoch": 0.053506054632497885,
      "grad_norm": 23.018688201904297,
      "learning_rate": 4.911057917957383e-05,
      "loss": 3.2677,
      "step": 380
    },
    {
      "epoch": 0.05421008166713602,
      "grad_norm": 39.09431838989258,
      "learning_rate": 4.909884539566319e-05,
      "loss": 3.5755,
      "step": 385
    },
    {
      "epoch": 0.05491410870177415,
      "grad_norm": 13.480724334716797,
      "learning_rate": 4.908711161175256e-05,
      "loss": 2.4528,
      "step": 390
    },
    {
      "epoch": 0.05561813573641228,
      "grad_norm": 37.701080322265625,
      "learning_rate": 4.907537782784192e-05,
      "loss": 3.593,
      "step": 395
    },
    {
      "epoch": 0.056322162771050406,
      "grad_norm": 20.265947341918945,
      "learning_rate": 4.906364404393129e-05,
      "loss": 2.8819,
      "step": 400
    },
    {
      "epoch": 0.05702618980568854,
      "grad_norm": 22.641456604003906,
      "learning_rate": 4.9051910260020654e-05,
      "loss": 3.2619,
      "step": 405
    },
    {
      "epoch": 0.05773021684032667,
      "grad_norm": 29.65068817138672,
      "learning_rate": 4.9040176476110016e-05,
      "loss": 4.3321,
      "step": 410
    },
    {
      "epoch": 0.0584342438749648,
      "grad_norm": 51.106285095214844,
      "learning_rate": 4.9028442692199385e-05,
      "loss": 3.5762,
      "step": 415
    },
    {
      "epoch": 0.05913827090960293,
      "grad_norm": 34.291500091552734,
      "learning_rate": 4.901670890828875e-05,
      "loss": 3.7583,
      "step": 420
    },
    {
      "epoch": 0.059842297944241056,
      "grad_norm": 35.47675704956055,
      "learning_rate": 4.900497512437811e-05,
      "loss": 3.463,
      "step": 425
    },
    {
      "epoch": 0.06054632497887919,
      "grad_norm": 30.262670516967773,
      "learning_rate": 4.899324134046748e-05,
      "loss": 2.9643,
      "step": 430
    },
    {
      "epoch": 0.06125035201351732,
      "grad_norm": 27.971546173095703,
      "learning_rate": 4.898150755655684e-05,
      "loss": 4.2147,
      "step": 435
    },
    {
      "epoch": 0.06195437904815545,
      "grad_norm": 25.399412155151367,
      "learning_rate": 4.89697737726462e-05,
      "loss": 2.5974,
      "step": 440
    },
    {
      "epoch": 0.06265840608279358,
      "grad_norm": 18.59703826904297,
      "learning_rate": 4.895803998873557e-05,
      "loss": 3.1693,
      "step": 445
    },
    {
      "epoch": 0.06336243311743171,
      "grad_norm": 20.097209930419922,
      "learning_rate": 4.894630620482493e-05,
      "loss": 3.4866,
      "step": 450
    },
    {
      "epoch": 0.06406646015206983,
      "grad_norm": 21.045867919921875,
      "learning_rate": 4.89345724209143e-05,
      "loss": 3.5145,
      "step": 455
    },
    {
      "epoch": 0.06477048718670797,
      "grad_norm": 21.93134117126465,
      "learning_rate": 4.892283863700366e-05,
      "loss": 2.7939,
      "step": 460
    },
    {
      "epoch": 0.0654745142213461,
      "grad_norm": 30.868257522583008,
      "learning_rate": 4.8911104853093025e-05,
      "loss": 3.0394,
      "step": 465
    },
    {
      "epoch": 0.06617854125598423,
      "grad_norm": 14.802631378173828,
      "learning_rate": 4.8899371069182394e-05,
      "loss": 2.6212,
      "step": 470
    },
    {
      "epoch": 0.06688256829062236,
      "grad_norm": 36.98556900024414,
      "learning_rate": 4.8887637285271756e-05,
      "loss": 3.0344,
      "step": 475
    },
    {
      "epoch": 0.06758659532526048,
      "grad_norm": 18.828088760375977,
      "learning_rate": 4.887590350136112e-05,
      "loss": 3.3488,
      "step": 480
    },
    {
      "epoch": 0.06829062235989862,
      "grad_norm": 16.3453369140625,
      "learning_rate": 4.886416971745049e-05,
      "loss": 3.3849,
      "step": 485
    },
    {
      "epoch": 0.06899464939453676,
      "grad_norm": 27.993064880371094,
      "learning_rate": 4.8852435933539855e-05,
      "loss": 4.4101,
      "step": 490
    },
    {
      "epoch": 0.06969867642917488,
      "grad_norm": 20.2993106842041,
      "learning_rate": 4.884070214962921e-05,
      "loss": 2.7406,
      "step": 495
    },
    {
      "epoch": 0.07040270346381301,
      "grad_norm": 23.21330451965332,
      "learning_rate": 4.882896836571858e-05,
      "loss": 2.4735,
      "step": 500
    },
    {
      "epoch": 0.07110673049845113,
      "grad_norm": 15.895278930664062,
      "learning_rate": 4.881723458180794e-05,
      "loss": 3.2241,
      "step": 505
    },
    {
      "epoch": 0.07181075753308927,
      "grad_norm": 12.11599063873291,
      "learning_rate": 4.880550079789731e-05,
      "loss": 3.1576,
      "step": 510
    },
    {
      "epoch": 0.0725147845677274,
      "grad_norm": 17.075084686279297,
      "learning_rate": 4.879376701398667e-05,
      "loss": 2.9803,
      "step": 515
    },
    {
      "epoch": 0.07321881160236553,
      "grad_norm": 21.234277725219727,
      "learning_rate": 4.8782033230076034e-05,
      "loss": 3.0282,
      "step": 520
    },
    {
      "epoch": 0.07392283863700366,
      "grad_norm": 20.003318786621094,
      "learning_rate": 4.87702994461654e-05,
      "loss": 3.4014,
      "step": 525
    },
    {
      "epoch": 0.07462686567164178,
      "grad_norm": 24.103443145751953,
      "learning_rate": 4.8758565662254765e-05,
      "loss": 2.7914,
      "step": 530
    },
    {
      "epoch": 0.07533089270627992,
      "grad_norm": 26.98702621459961,
      "learning_rate": 4.874683187834413e-05,
      "loss": 3.5042,
      "step": 535
    },
    {
      "epoch": 0.07603491974091806,
      "grad_norm": 23.8365421295166,
      "learning_rate": 4.8735098094433496e-05,
      "loss": 3.3161,
      "step": 540
    },
    {
      "epoch": 0.07673894677555618,
      "grad_norm": 13.927907943725586,
      "learning_rate": 4.8723364310522865e-05,
      "loss": 4.2731,
      "step": 545
    },
    {
      "epoch": 0.07744297381019431,
      "grad_norm": 15.653139114379883,
      "learning_rate": 4.871163052661222e-05,
      "loss": 3.08,
      "step": 550
    },
    {
      "epoch": 0.07814700084483245,
      "grad_norm": 12.53940486907959,
      "learning_rate": 4.869989674270159e-05,
      "loss": 3.4197,
      "step": 555
    },
    {
      "epoch": 0.07885102787947057,
      "grad_norm": 29.274282455444336,
      "learning_rate": 4.868816295879095e-05,
      "loss": 3.6087,
      "step": 560
    },
    {
      "epoch": 0.0795550549141087,
      "grad_norm": 12.94975471496582,
      "learning_rate": 4.867642917488032e-05,
      "loss": 3.2621,
      "step": 565
    },
    {
      "epoch": 0.08025908194874683,
      "grad_norm": 15.587959289550781,
      "learning_rate": 4.866469539096968e-05,
      "loss": 3.0031,
      "step": 570
    },
    {
      "epoch": 0.08096310898338496,
      "grad_norm": 22.914878845214844,
      "learning_rate": 4.865296160705904e-05,
      "loss": 3.4214,
      "step": 575
    },
    {
      "epoch": 0.0816671360180231,
      "grad_norm": 26.60297203063965,
      "learning_rate": 4.864122782314841e-05,
      "loss": 2.5418,
      "step": 580
    },
    {
      "epoch": 0.08237116305266122,
      "grad_norm": 30.719402313232422,
      "learning_rate": 4.8629494039237774e-05,
      "loss": 3.0865,
      "step": 585
    },
    {
      "epoch": 0.08307519008729936,
      "grad_norm": 13.865996360778809,
      "learning_rate": 4.8617760255327136e-05,
      "loss": 3.3664,
      "step": 590
    },
    {
      "epoch": 0.08377921712193748,
      "grad_norm": 32.514808654785156,
      "learning_rate": 4.8606026471416505e-05,
      "loss": 2.7984,
      "step": 595
    },
    {
      "epoch": 0.08448324415657561,
      "grad_norm": 28.505146026611328,
      "learning_rate": 4.8594292687505874e-05,
      "loss": 3.7105,
      "step": 600
    },
    {
      "epoch": 0.08518727119121375,
      "grad_norm": 16.74730682373047,
      "learning_rate": 4.858255890359523e-05,
      "loss": 2.8659,
      "step": 605
    },
    {
      "epoch": 0.08589129822585187,
      "grad_norm": 16.48188591003418,
      "learning_rate": 4.85708251196846e-05,
      "loss": 3.3243,
      "step": 610
    },
    {
      "epoch": 0.08659532526049,
      "grad_norm": 13.029666900634766,
      "learning_rate": 4.8559091335773966e-05,
      "loss": 3.1198,
      "step": 615
    },
    {
      "epoch": 0.08729935229512813,
      "grad_norm": 14.608738899230957,
      "learning_rate": 4.854735755186333e-05,
      "loss": 2.99,
      "step": 620
    },
    {
      "epoch": 0.08800337932976626,
      "grad_norm": 16.359201431274414,
      "learning_rate": 4.853562376795269e-05,
      "loss": 2.445,
      "step": 625
    },
    {
      "epoch": 0.0887074063644044,
      "grad_norm": 12.290416717529297,
      "learning_rate": 4.852388998404205e-05,
      "loss": 3.089,
      "step": 630
    },
    {
      "epoch": 0.08941143339904252,
      "grad_norm": 17.04356575012207,
      "learning_rate": 4.851215620013142e-05,
      "loss": 3.5101,
      "step": 635
    },
    {
      "epoch": 0.09011546043368066,
      "grad_norm": 12.944172859191895,
      "learning_rate": 4.850042241622078e-05,
      "loss": 3.2746,
      "step": 640
    },
    {
      "epoch": 0.09081948746831878,
      "grad_norm": 16.50133514404297,
      "learning_rate": 4.8488688632310145e-05,
      "loss": 3.6892,
      "step": 645
    },
    {
      "epoch": 0.09152351450295691,
      "grad_norm": 15.733154296875,
      "learning_rate": 4.8476954848399514e-05,
      "loss": 3.1589,
      "step": 650
    },
    {
      "epoch": 0.09222754153759505,
      "grad_norm": 16.360931396484375,
      "learning_rate": 4.846522106448888e-05,
      "loss": 3.4306,
      "step": 655
    },
    {
      "epoch": 0.09293156857223317,
      "grad_norm": 18.856098175048828,
      "learning_rate": 4.845348728057824e-05,
      "loss": 3.2314,
      "step": 660
    },
    {
      "epoch": 0.0936355956068713,
      "grad_norm": 10.125259399414062,
      "learning_rate": 4.844175349666761e-05,
      "loss": 3.4591,
      "step": 665
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 23.869762420654297,
      "learning_rate": 4.8430019712756976e-05,
      "loss": 3.2582,
      "step": 670
    },
    {
      "epoch": 0.09504364967614756,
      "grad_norm": 23.59833526611328,
      "learning_rate": 4.841828592884634e-05,
      "loss": 3.6508,
      "step": 675
    },
    {
      "epoch": 0.0957476767107857,
      "grad_norm": 37.445716857910156,
      "learning_rate": 4.84065521449357e-05,
      "loss": 3.0261,
      "step": 680
    },
    {
      "epoch": 0.09645170374542382,
      "grad_norm": 18.451068878173828,
      "learning_rate": 4.839481836102507e-05,
      "loss": 2.3082,
      "step": 685
    },
    {
      "epoch": 0.09715573078006196,
      "grad_norm": 11.276684761047363,
      "learning_rate": 4.838308457711443e-05,
      "loss": 2.7082,
      "step": 690
    },
    {
      "epoch": 0.09785975781470009,
      "grad_norm": 47.281307220458984,
      "learning_rate": 4.837135079320379e-05,
      "loss": 3.2901,
      "step": 695
    },
    {
      "epoch": 0.09856378484933821,
      "grad_norm": 17.969083786010742,
      "learning_rate": 4.8359617009293154e-05,
      "loss": 3.0207,
      "step": 700
    },
    {
      "epoch": 0.09926781188397635,
      "grad_norm": 18.608318328857422,
      "learning_rate": 4.834788322538252e-05,
      "loss": 2.8389,
      "step": 705
    },
    {
      "epoch": 0.09997183891861447,
      "grad_norm": 14.267355918884277,
      "learning_rate": 4.833614944147189e-05,
      "loss": 3.2999,
      "step": 710
    },
    {
      "epoch": 0.1006758659532526,
      "grad_norm": 13.156573295593262,
      "learning_rate": 4.8324415657561254e-05,
      "loss": 3.2965,
      "step": 715
    },
    {
      "epoch": 0.10137989298789074,
      "grad_norm": 21.06451988220215,
      "learning_rate": 4.8312681873650616e-05,
      "loss": 2.8538,
      "step": 720
    },
    {
      "epoch": 0.10208392002252886,
      "grad_norm": 13.592069625854492,
      "learning_rate": 4.8300948089739985e-05,
      "loss": 2.7547,
      "step": 725
    },
    {
      "epoch": 0.102787947057167,
      "grad_norm": 24.145631790161133,
      "learning_rate": 4.828921430582935e-05,
      "loss": 3.4342,
      "step": 730
    },
    {
      "epoch": 0.10349197409180512,
      "grad_norm": 15.570745468139648,
      "learning_rate": 4.827748052191871e-05,
      "loss": 2.8276,
      "step": 735
    },
    {
      "epoch": 0.10419600112644326,
      "grad_norm": 24.374231338500977,
      "learning_rate": 4.826574673800808e-05,
      "loss": 3.2609,
      "step": 740
    },
    {
      "epoch": 0.10490002816108139,
      "grad_norm": 27.88130760192871,
      "learning_rate": 4.825401295409744e-05,
      "loss": 3.1529,
      "step": 745
    },
    {
      "epoch": 0.10560405519571951,
      "grad_norm": 20.022361755371094,
      "learning_rate": 4.824227917018681e-05,
      "loss": 2.8593,
      "step": 750
    },
    {
      "epoch": 0.10630808223035765,
      "grad_norm": 12.54682731628418,
      "learning_rate": 4.823054538627617e-05,
      "loss": 3.3018,
      "step": 755
    },
    {
      "epoch": 0.10701210926499577,
      "grad_norm": 16.840002059936523,
      "learning_rate": 4.821881160236553e-05,
      "loss": 3.365,
      "step": 760
    },
    {
      "epoch": 0.1077161362996339,
      "grad_norm": 13.647838592529297,
      "learning_rate": 4.82070778184549e-05,
      "loss": 3.0997,
      "step": 765
    },
    {
      "epoch": 0.10842016333427204,
      "grad_norm": 22.849618911743164,
      "learning_rate": 4.819534403454426e-05,
      "loss": 3.001,
      "step": 770
    },
    {
      "epoch": 0.10912419036891016,
      "grad_norm": 10.08050537109375,
      "learning_rate": 4.8183610250633625e-05,
      "loss": 2.7797,
      "step": 775
    },
    {
      "epoch": 0.1098282174035483,
      "grad_norm": 22.945663452148438,
      "learning_rate": 4.8171876466722994e-05,
      "loss": 3.9331,
      "step": 780
    },
    {
      "epoch": 0.11053224443818642,
      "grad_norm": 12.948235511779785,
      "learning_rate": 4.8160142682812356e-05,
      "loss": 3.527,
      "step": 785
    },
    {
      "epoch": 0.11123627147282455,
      "grad_norm": 13.94672966003418,
      "learning_rate": 4.814840889890172e-05,
      "loss": 2.8931,
      "step": 790
    },
    {
      "epoch": 0.11194029850746269,
      "grad_norm": 16.577491760253906,
      "learning_rate": 4.8136675114991087e-05,
      "loss": 3.2746,
      "step": 795
    },
    {
      "epoch": 0.11264432554210081,
      "grad_norm": 22.389347076416016,
      "learning_rate": 4.812494133108045e-05,
      "loss": 3.0427,
      "step": 800
    },
    {
      "epoch": 0.11334835257673895,
      "grad_norm": 8.099717140197754,
      "learning_rate": 4.811320754716982e-05,
      "loss": 3.0286,
      "step": 805
    },
    {
      "epoch": 0.11405237961137708,
      "grad_norm": 19.037742614746094,
      "learning_rate": 4.810147376325918e-05,
      "loss": 2.9163,
      "step": 810
    },
    {
      "epoch": 0.1147564066460152,
      "grad_norm": 14.134026527404785,
      "learning_rate": 4.808973997934854e-05,
      "loss": 3.5401,
      "step": 815
    },
    {
      "epoch": 0.11546043368065334,
      "grad_norm": 25.1518497467041,
      "learning_rate": 4.807800619543791e-05,
      "loss": 3.388,
      "step": 820
    },
    {
      "epoch": 0.11616446071529146,
      "grad_norm": 19.19191551208496,
      "learning_rate": 4.806627241152727e-05,
      "loss": 3.3321,
      "step": 825
    },
    {
      "epoch": 0.1168684877499296,
      "grad_norm": 14.028707504272461,
      "learning_rate": 4.8054538627616634e-05,
      "loss": 2.7382,
      "step": 830
    },
    {
      "epoch": 0.11757251478456773,
      "grad_norm": 12.277022361755371,
      "learning_rate": 4.8042804843706e-05,
      "loss": 3.7079,
      "step": 835
    },
    {
      "epoch": 0.11827654181920585,
      "grad_norm": 19.385103225708008,
      "learning_rate": 4.8031071059795365e-05,
      "loss": 3.3168,
      "step": 840
    },
    {
      "epoch": 0.11898056885384399,
      "grad_norm": 12.704506874084473,
      "learning_rate": 4.801933727588473e-05,
      "loss": 3.1625,
      "step": 845
    },
    {
      "epoch": 0.11968459588848211,
      "grad_norm": 14.645435333251953,
      "learning_rate": 4.8007603491974096e-05,
      "loss": 2.4791,
      "step": 850
    },
    {
      "epoch": 0.12038862292312025,
      "grad_norm": 13.387099266052246,
      "learning_rate": 4.799586970806346e-05,
      "loss": 2.7933,
      "step": 855
    },
    {
      "epoch": 0.12109264995775838,
      "grad_norm": 14.123462677001953,
      "learning_rate": 4.7984135924152826e-05,
      "loss": 3.3894,
      "step": 860
    },
    {
      "epoch": 0.1217966769923965,
      "grad_norm": 17.2509822845459,
      "learning_rate": 4.797240214024219e-05,
      "loss": 2.6015,
      "step": 865
    },
    {
      "epoch": 0.12250070402703464,
      "grad_norm": 18.414859771728516,
      "learning_rate": 4.796066835633155e-05,
      "loss": 2.8746,
      "step": 870
    },
    {
      "epoch": 0.12320473106167276,
      "grad_norm": 11.913764953613281,
      "learning_rate": 4.794893457242092e-05,
      "loss": 2.7854,
      "step": 875
    },
    {
      "epoch": 0.1239087580963109,
      "grad_norm": 9.875001907348633,
      "learning_rate": 4.793720078851028e-05,
      "loss": 3.914,
      "step": 880
    },
    {
      "epoch": 0.12461278513094903,
      "grad_norm": 12.77676010131836,
      "learning_rate": 4.792546700459964e-05,
      "loss": 2.4366,
      "step": 885
    },
    {
      "epoch": 0.12531681216558715,
      "grad_norm": 12.020976066589355,
      "learning_rate": 4.791373322068901e-05,
      "loss": 3.2367,
      "step": 890
    },
    {
      "epoch": 0.12602083920022528,
      "grad_norm": 24.02552604675293,
      "learning_rate": 4.7901999436778374e-05,
      "loss": 4.1596,
      "step": 895
    },
    {
      "epoch": 0.12672486623486343,
      "grad_norm": 13.685201644897461,
      "learning_rate": 4.7890265652867736e-05,
      "loss": 3.2537,
      "step": 900
    },
    {
      "epoch": 0.12742889326950155,
      "grad_norm": 12.17990493774414,
      "learning_rate": 4.7878531868957105e-05,
      "loss": 2.3784,
      "step": 905
    },
    {
      "epoch": 0.12813292030413967,
      "grad_norm": 16.966716766357422,
      "learning_rate": 4.786679808504647e-05,
      "loss": 3.0084,
      "step": 910
    },
    {
      "epoch": 0.12883694733877782,
      "grad_norm": 12.076171875,
      "learning_rate": 4.7855064301135836e-05,
      "loss": 3.0116,
      "step": 915
    },
    {
      "epoch": 0.12954097437341594,
      "grad_norm": 41.84965896606445,
      "learning_rate": 4.78433305172252e-05,
      "loss": 3.1598,
      "step": 920
    },
    {
      "epoch": 0.13024500140805406,
      "grad_norm": 14.789374351501465,
      "learning_rate": 4.783159673331456e-05,
      "loss": 2.6583,
      "step": 925
    },
    {
      "epoch": 0.1309490284426922,
      "grad_norm": 16.76061248779297,
      "learning_rate": 4.781986294940393e-05,
      "loss": 2.8475,
      "step": 930
    },
    {
      "epoch": 0.13165305547733033,
      "grad_norm": 17.148841857910156,
      "learning_rate": 4.780812916549329e-05,
      "loss": 2.8152,
      "step": 935
    },
    {
      "epoch": 0.13235708251196845,
      "grad_norm": 14.747498512268066,
      "learning_rate": 4.779639538158265e-05,
      "loss": 3.4942,
      "step": 940
    },
    {
      "epoch": 0.13306110954660658,
      "grad_norm": 22.67784881591797,
      "learning_rate": 4.778466159767202e-05,
      "loss": 3.3157,
      "step": 945
    },
    {
      "epoch": 0.13376513658124473,
      "grad_norm": 21.867591857910156,
      "learning_rate": 4.777292781376139e-05,
      "loss": 2.4206,
      "step": 950
    },
    {
      "epoch": 0.13446916361588285,
      "grad_norm": 18.663454055786133,
      "learning_rate": 4.7761194029850745e-05,
      "loss": 3.8378,
      "step": 955
    },
    {
      "epoch": 0.13517319065052097,
      "grad_norm": 4.493278503417969,
      "learning_rate": 4.7749460245940114e-05,
      "loss": 2.8268,
      "step": 960
    },
    {
      "epoch": 0.13587721768515912,
      "grad_norm": 17.350467681884766,
      "learning_rate": 4.7737726462029476e-05,
      "loss": 3.1067,
      "step": 965
    },
    {
      "epoch": 0.13658124471979724,
      "grad_norm": 10.186059951782227,
      "learning_rate": 4.7725992678118845e-05,
      "loss": 3.3651,
      "step": 970
    },
    {
      "epoch": 0.13728527175443536,
      "grad_norm": 14.301260948181152,
      "learning_rate": 4.771425889420821e-05,
      "loss": 3.6926,
      "step": 975
    },
    {
      "epoch": 0.1379892987890735,
      "grad_norm": 14.852526664733887,
      "learning_rate": 4.770252511029757e-05,
      "loss": 3.6057,
      "step": 980
    },
    {
      "epoch": 0.13869332582371163,
      "grad_norm": 15.638705253601074,
      "learning_rate": 4.769079132638694e-05,
      "loss": 3.1245,
      "step": 985
    },
    {
      "epoch": 0.13939735285834975,
      "grad_norm": 9.794548988342285,
      "learning_rate": 4.76790575424763e-05,
      "loss": 2.7087,
      "step": 990
    },
    {
      "epoch": 0.1401013798929879,
      "grad_norm": 6.292840480804443,
      "learning_rate": 4.766732375856566e-05,
      "loss": 3.2254,
      "step": 995
    },
    {
      "epoch": 0.14080540692762603,
      "grad_norm": 13.042623519897461,
      "learning_rate": 4.765558997465503e-05,
      "loss": 3.1858,
      "step": 1000
    },
    {
      "epoch": 0.14150943396226415,
      "grad_norm": 16.836387634277344,
      "learning_rate": 4.76438561907444e-05,
      "loss": 3.7769,
      "step": 1005
    },
    {
      "epoch": 0.14221346099690227,
      "grad_norm": 19.096508026123047,
      "learning_rate": 4.7632122406833754e-05,
      "loss": 2.7769,
      "step": 1010
    },
    {
      "epoch": 0.14291748803154042,
      "grad_norm": 9.030710220336914,
      "learning_rate": 4.762038862292312e-05,
      "loss": 2.8141,
      "step": 1015
    },
    {
      "epoch": 0.14362151506617854,
      "grad_norm": 20.348051071166992,
      "learning_rate": 4.760865483901249e-05,
      "loss": 2.9491,
      "step": 1020
    },
    {
      "epoch": 0.14432554210081666,
      "grad_norm": 14.519989967346191,
      "learning_rate": 4.7596921055101854e-05,
      "loss": 3.273,
      "step": 1025
    },
    {
      "epoch": 0.1450295691354548,
      "grad_norm": 13.52905559539795,
      "learning_rate": 4.7585187271191216e-05,
      "loss": 2.6955,
      "step": 1030
    },
    {
      "epoch": 0.14573359617009293,
      "grad_norm": 13.733450889587402,
      "learning_rate": 4.757345348728058e-05,
      "loss": 3.1128,
      "step": 1035
    },
    {
      "epoch": 0.14643762320473105,
      "grad_norm": 25.482698440551758,
      "learning_rate": 4.7561719703369947e-05,
      "loss": 3.6081,
      "step": 1040
    },
    {
      "epoch": 0.1471416502393692,
      "grad_norm": 12.162196159362793,
      "learning_rate": 4.754998591945931e-05,
      "loss": 2.6848,
      "step": 1045
    },
    {
      "epoch": 0.14784567727400733,
      "grad_norm": 16.064104080200195,
      "learning_rate": 4.753825213554867e-05,
      "loss": 2.8621,
      "step": 1050
    },
    {
      "epoch": 0.14854970430864545,
      "grad_norm": 12.651463508605957,
      "learning_rate": 4.752651835163804e-05,
      "loss": 3.2957,
      "step": 1055
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 17.72707748413086,
      "learning_rate": 4.751478456772741e-05,
      "loss": 3.4354,
      "step": 1060
    },
    {
      "epoch": 0.14995775837792172,
      "grad_norm": 15.178136825561523,
      "learning_rate": 4.750305078381676e-05,
      "loss": 2.6387,
      "step": 1065
    },
    {
      "epoch": 0.15066178541255984,
      "grad_norm": 10.757913589477539,
      "learning_rate": 4.749131699990613e-05,
      "loss": 3.0986,
      "step": 1070
    },
    {
      "epoch": 0.15136581244719796,
      "grad_norm": 7.8435492515563965,
      "learning_rate": 4.74795832159955e-05,
      "loss": 2.9368,
      "step": 1075
    },
    {
      "epoch": 0.1520698394818361,
      "grad_norm": 10.539892196655273,
      "learning_rate": 4.746784943208486e-05,
      "loss": 2.5262,
      "step": 1080
    },
    {
      "epoch": 0.15277386651647423,
      "grad_norm": 15.339691162109375,
      "learning_rate": 4.7456115648174225e-05,
      "loss": 2.5166,
      "step": 1085
    },
    {
      "epoch": 0.15347789355111235,
      "grad_norm": 15.605856895446777,
      "learning_rate": 4.744438186426359e-05,
      "loss": 3.128,
      "step": 1090
    },
    {
      "epoch": 0.1541819205857505,
      "grad_norm": 10.626863479614258,
      "learning_rate": 4.7432648080352956e-05,
      "loss": 3.7037,
      "step": 1095
    },
    {
      "epoch": 0.15488594762038863,
      "grad_norm": 17.20867919921875,
      "learning_rate": 4.742091429644232e-05,
      "loss": 2.841,
      "step": 1100
    },
    {
      "epoch": 0.15558997465502675,
      "grad_norm": 22.471637725830078,
      "learning_rate": 4.740918051253168e-05,
      "loss": 3.3949,
      "step": 1105
    },
    {
      "epoch": 0.1562940016896649,
      "grad_norm": 14.746288299560547,
      "learning_rate": 4.739744672862105e-05,
      "loss": 3.1784,
      "step": 1110
    },
    {
      "epoch": 0.15699802872430302,
      "grad_norm": 8.859037399291992,
      "learning_rate": 4.738571294471042e-05,
      "loss": 2.4247,
      "step": 1115
    },
    {
      "epoch": 0.15770205575894114,
      "grad_norm": 23.62889862060547,
      "learning_rate": 4.737397916079977e-05,
      "loss": 2.2083,
      "step": 1120
    },
    {
      "epoch": 0.15840608279357926,
      "grad_norm": 12.035709381103516,
      "learning_rate": 4.736224537688914e-05,
      "loss": 2.3393,
      "step": 1125
    },
    {
      "epoch": 0.1591101098282174,
      "grad_norm": 20.65583610534668,
      "learning_rate": 4.735051159297851e-05,
      "loss": 3.7502,
      "step": 1130
    },
    {
      "epoch": 0.15981413686285553,
      "grad_norm": 19.937789916992188,
      "learning_rate": 4.733877780906787e-05,
      "loss": 2.9793,
      "step": 1135
    },
    {
      "epoch": 0.16051816389749365,
      "grad_norm": 7.498140811920166,
      "learning_rate": 4.7327044025157234e-05,
      "loss": 3.4663,
      "step": 1140
    },
    {
      "epoch": 0.1612221909321318,
      "grad_norm": 15.351911544799805,
      "learning_rate": 4.73153102412466e-05,
      "loss": 3.0078,
      "step": 1145
    },
    {
      "epoch": 0.16192621796676993,
      "grad_norm": 7.629034042358398,
      "learning_rate": 4.7303576457335965e-05,
      "loss": 2.9108,
      "step": 1150
    },
    {
      "epoch": 0.16263024500140805,
      "grad_norm": 13.656631469726562,
      "learning_rate": 4.729184267342533e-05,
      "loss": 2.2651,
      "step": 1155
    },
    {
      "epoch": 0.1633342720360462,
      "grad_norm": 13.419894218444824,
      "learning_rate": 4.728010888951469e-05,
      "loss": 2.8317,
      "step": 1160
    },
    {
      "epoch": 0.16403829907068432,
      "grad_norm": 9.516971588134766,
      "learning_rate": 4.726837510560406e-05,
      "loss": 2.8955,
      "step": 1165
    },
    {
      "epoch": 0.16474232610532244,
      "grad_norm": 13.624449729919434,
      "learning_rate": 4.7256641321693426e-05,
      "loss": 3.2084,
      "step": 1170
    },
    {
      "epoch": 0.16544635313996056,
      "grad_norm": 11.256967544555664,
      "learning_rate": 4.724490753778278e-05,
      "loss": 3.2233,
      "step": 1175
    },
    {
      "epoch": 0.1661503801745987,
      "grad_norm": 19.809146881103516,
      "learning_rate": 4.723317375387215e-05,
      "loss": 3.1925,
      "step": 1180
    },
    {
      "epoch": 0.16685440720923683,
      "grad_norm": 10.025758743286133,
      "learning_rate": 4.722143996996152e-05,
      "loss": 2.5668,
      "step": 1185
    },
    {
      "epoch": 0.16755843424387495,
      "grad_norm": 11.326897621154785,
      "learning_rate": 4.720970618605088e-05,
      "loss": 2.6314,
      "step": 1190
    },
    {
      "epoch": 0.1682624612785131,
      "grad_norm": 17.730979919433594,
      "learning_rate": 4.719797240214024e-05,
      "loss": 3.1509,
      "step": 1195
    },
    {
      "epoch": 0.16896648831315123,
      "grad_norm": 29.750553131103516,
      "learning_rate": 4.718623861822961e-05,
      "loss": 3.2835,
      "step": 1200
    },
    {
      "epoch": 0.16967051534778935,
      "grad_norm": 12.092218399047852,
      "learning_rate": 4.7174504834318974e-05,
      "loss": 3.0938,
      "step": 1205
    },
    {
      "epoch": 0.1703745423824275,
      "grad_norm": 14.575237274169922,
      "learning_rate": 4.7162771050408336e-05,
      "loss": 3.3084,
      "step": 1210
    },
    {
      "epoch": 0.17107856941706562,
      "grad_norm": 8.487334251403809,
      "learning_rate": 4.7151037266497705e-05,
      "loss": 3.5931,
      "step": 1215
    },
    {
      "epoch": 0.17178259645170374,
      "grad_norm": 21.5622501373291,
      "learning_rate": 4.713930348258707e-05,
      "loss": 3.684,
      "step": 1220
    },
    {
      "epoch": 0.1724866234863419,
      "grad_norm": 10.9602632522583,
      "learning_rate": 4.7127569698676435e-05,
      "loss": 3.6722,
      "step": 1225
    },
    {
      "epoch": 0.17319065052098,
      "grad_norm": 10.618412017822266,
      "learning_rate": 4.711583591476579e-05,
      "loss": 2.9598,
      "step": 1230
    },
    {
      "epoch": 0.17389467755561813,
      "grad_norm": 21.156635284423828,
      "learning_rate": 4.710410213085516e-05,
      "loss": 2.9809,
      "step": 1235
    },
    {
      "epoch": 0.17459870459025625,
      "grad_norm": 11.064668655395508,
      "learning_rate": 4.709236834694453e-05,
      "loss": 2.4342,
      "step": 1240
    },
    {
      "epoch": 0.1753027316248944,
      "grad_norm": 11.969633102416992,
      "learning_rate": 4.708063456303389e-05,
      "loss": 3.0803,
      "step": 1245
    },
    {
      "epoch": 0.17600675865953253,
      "grad_norm": 19.545976638793945,
      "learning_rate": 4.706890077912325e-05,
      "loss": 3.4984,
      "step": 1250
    },
    {
      "epoch": 0.17671078569417065,
      "grad_norm": 7.6093902587890625,
      "learning_rate": 4.705716699521262e-05,
      "loss": 3.2297,
      "step": 1255
    },
    {
      "epoch": 0.1774148127288088,
      "grad_norm": 9.54875373840332,
      "learning_rate": 4.704543321130198e-05,
      "loss": 3.0602,
      "step": 1260
    },
    {
      "epoch": 0.17811883976344692,
      "grad_norm": 11.275030136108398,
      "learning_rate": 4.7033699427391345e-05,
      "loss": 3.7198,
      "step": 1265
    },
    {
      "epoch": 0.17882286679808504,
      "grad_norm": 9.006635665893555,
      "learning_rate": 4.7021965643480714e-05,
      "loss": 3.2625,
      "step": 1270
    },
    {
      "epoch": 0.1795268938327232,
      "grad_norm": 23.366785049438477,
      "learning_rate": 4.7010231859570076e-05,
      "loss": 3.0554,
      "step": 1275
    },
    {
      "epoch": 0.1802309208673613,
      "grad_norm": 3.093703508377075,
      "learning_rate": 4.6998498075659445e-05,
      "loss": 2.8083,
      "step": 1280
    },
    {
      "epoch": 0.18093494790199943,
      "grad_norm": 9.853776931762695,
      "learning_rate": 4.69867642917488e-05,
      "loss": 2.1874,
      "step": 1285
    },
    {
      "epoch": 0.18163897493663755,
      "grad_norm": 26.672508239746094,
      "learning_rate": 4.697503050783817e-05,
      "loss": 3.7943,
      "step": 1290
    },
    {
      "epoch": 0.1823430019712757,
      "grad_norm": 11.67150592803955,
      "learning_rate": 4.696329672392754e-05,
      "loss": 2.9037,
      "step": 1295
    },
    {
      "epoch": 0.18304702900591382,
      "grad_norm": 29.4053897857666,
      "learning_rate": 4.69515629400169e-05,
      "loss": 3.1072,
      "step": 1300
    },
    {
      "epoch": 0.18375105604055195,
      "grad_norm": 14.364642143249512,
      "learning_rate": 4.693982915610626e-05,
      "loss": 1.9319,
      "step": 1305
    },
    {
      "epoch": 0.1844550830751901,
      "grad_norm": 14.353659629821777,
      "learning_rate": 4.692809537219563e-05,
      "loss": 1.5772,
      "step": 1310
    },
    {
      "epoch": 0.18515911010982822,
      "grad_norm": 18.711454391479492,
      "learning_rate": 4.691636158828499e-05,
      "loss": 3.2433,
      "step": 1315
    },
    {
      "epoch": 0.18586313714446634,
      "grad_norm": 10.619996070861816,
      "learning_rate": 4.6904627804374354e-05,
      "loss": 2.4326,
      "step": 1320
    },
    {
      "epoch": 0.1865671641791045,
      "grad_norm": 10.510001182556152,
      "learning_rate": 4.689289402046372e-05,
      "loss": 2.1189,
      "step": 1325
    },
    {
      "epoch": 0.1872711912137426,
      "grad_norm": 8.74335765838623,
      "learning_rate": 4.6881160236553085e-05,
      "loss": 2.205,
      "step": 1330
    },
    {
      "epoch": 0.18797521824838073,
      "grad_norm": 13.692355155944824,
      "learning_rate": 4.6869426452642454e-05,
      "loss": 2.5625,
      "step": 1335
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 9.15758228302002,
      "learning_rate": 4.6857692668731816e-05,
      "loss": 2.6706,
      "step": 1340
    },
    {
      "epoch": 0.189383272317657,
      "grad_norm": 12.543216705322266,
      "learning_rate": 4.684595888482118e-05,
      "loss": 2.1296,
      "step": 1345
    },
    {
      "epoch": 0.19008729935229512,
      "grad_norm": 17.624919891357422,
      "learning_rate": 4.6834225100910546e-05,
      "loss": 2.6356,
      "step": 1350
    },
    {
      "epoch": 0.19079132638693325,
      "grad_norm": 20.724245071411133,
      "learning_rate": 4.682249131699991e-05,
      "loss": 2.7094,
      "step": 1355
    },
    {
      "epoch": 0.1914953534215714,
      "grad_norm": 10.03364086151123,
      "learning_rate": 4.681075753308927e-05,
      "loss": 3.0197,
      "step": 1360
    },
    {
      "epoch": 0.19219938045620952,
      "grad_norm": 13.009559631347656,
      "learning_rate": 4.679902374917864e-05,
      "loss": 3.5852,
      "step": 1365
    },
    {
      "epoch": 0.19290340749084764,
      "grad_norm": 22.91817855834961,
      "learning_rate": 4.6787289965268e-05,
      "loss": 2.6077,
      "step": 1370
    },
    {
      "epoch": 0.1936074345254858,
      "grad_norm": 15.14262866973877,
      "learning_rate": 4.677555618135736e-05,
      "loss": 3.2059,
      "step": 1375
    },
    {
      "epoch": 0.1943114615601239,
      "grad_norm": 20.89275360107422,
      "learning_rate": 4.676382239744673e-05,
      "loss": 3.8312,
      "step": 1380
    },
    {
      "epoch": 0.19501548859476203,
      "grad_norm": 10.431924819946289,
      "learning_rate": 4.6752088613536094e-05,
      "loss": 2.0622,
      "step": 1385
    },
    {
      "epoch": 0.19571951562940018,
      "grad_norm": 15.958020210266113,
      "learning_rate": 4.674035482962546e-05,
      "loss": 2.7015,
      "step": 1390
    },
    {
      "epoch": 0.1964235426640383,
      "grad_norm": 8.839923858642578,
      "learning_rate": 4.6728621045714825e-05,
      "loss": 2.0117,
      "step": 1395
    },
    {
      "epoch": 0.19712756969867642,
      "grad_norm": 15.782098770141602,
      "learning_rate": 4.671688726180419e-05,
      "loss": 2.6079,
      "step": 1400
    },
    {
      "epoch": 0.19783159673331455,
      "grad_norm": 10.296194076538086,
      "learning_rate": 4.6705153477893556e-05,
      "loss": 2.9357,
      "step": 1405
    },
    {
      "epoch": 0.1985356237679527,
      "grad_norm": 17.415462493896484,
      "learning_rate": 4.669341969398292e-05,
      "loss": 2.7319,
      "step": 1410
    },
    {
      "epoch": 0.19923965080259082,
      "grad_norm": 15.512150764465332,
      "learning_rate": 4.668168591007228e-05,
      "loss": 3.247,
      "step": 1415
    },
    {
      "epoch": 0.19994367783722894,
      "grad_norm": 14.63789176940918,
      "learning_rate": 4.666995212616165e-05,
      "loss": 3.378,
      "step": 1420
    },
    {
      "epoch": 0.2006477048718671,
      "grad_norm": 18.008359909057617,
      "learning_rate": 4.665821834225101e-05,
      "loss": 3.1987,
      "step": 1425
    },
    {
      "epoch": 0.2013517319065052,
      "grad_norm": 13.688739776611328,
      "learning_rate": 4.664648455834037e-05,
      "loss": 3.0355,
      "step": 1430
    },
    {
      "epoch": 0.20205575894114333,
      "grad_norm": 22.49468994140625,
      "learning_rate": 4.663475077442974e-05,
      "loss": 2.5644,
      "step": 1435
    },
    {
      "epoch": 0.20275978597578148,
      "grad_norm": 10.658085823059082,
      "learning_rate": 4.66230169905191e-05,
      "loss": 2.7691,
      "step": 1440
    },
    {
      "epoch": 0.2034638130104196,
      "grad_norm": 9.203879356384277,
      "learning_rate": 4.661128320660847e-05,
      "loss": 2.0664,
      "step": 1445
    },
    {
      "epoch": 0.20416784004505772,
      "grad_norm": 12.093940734863281,
      "learning_rate": 4.6599549422697834e-05,
      "loss": 2.1997,
      "step": 1450
    },
    {
      "epoch": 0.20487186707969587,
      "grad_norm": 24.551713943481445,
      "learning_rate": 4.6587815638787196e-05,
      "loss": 2.1218,
      "step": 1455
    },
    {
      "epoch": 0.205575894114334,
      "grad_norm": 7.478397846221924,
      "learning_rate": 4.6576081854876565e-05,
      "loss": 2.552,
      "step": 1460
    },
    {
      "epoch": 0.20627992114897212,
      "grad_norm": 13.2855806350708,
      "learning_rate": 4.656434807096593e-05,
      "loss": 3.1971,
      "step": 1465
    },
    {
      "epoch": 0.20698394818361024,
      "grad_norm": 12.5995454788208,
      "learning_rate": 4.655261428705529e-05,
      "loss": 2.7422,
      "step": 1470
    },
    {
      "epoch": 0.2076879752182484,
      "grad_norm": 22.975419998168945,
      "learning_rate": 4.654088050314466e-05,
      "loss": 2.9326,
      "step": 1475
    },
    {
      "epoch": 0.2083920022528865,
      "grad_norm": 9.496504783630371,
      "learning_rate": 4.6529146719234026e-05,
      "loss": 3.0511,
      "step": 1480
    },
    {
      "epoch": 0.20909602928752463,
      "grad_norm": 13.818631172180176,
      "learning_rate": 4.651741293532338e-05,
      "loss": 2.9722,
      "step": 1485
    },
    {
      "epoch": 0.20980005632216278,
      "grad_norm": 13.794036865234375,
      "learning_rate": 4.650567915141275e-05,
      "loss": 2.8406,
      "step": 1490
    },
    {
      "epoch": 0.2105040833568009,
      "grad_norm": 11.437505722045898,
      "learning_rate": 4.649394536750211e-05,
      "loss": 2.5565,
      "step": 1495
    },
    {
      "epoch": 0.21120811039143902,
      "grad_norm": 10.330491065979004,
      "learning_rate": 4.648221158359148e-05,
      "loss": 2.9453,
      "step": 1500
    },
    {
      "epoch": 0.21191213742607717,
      "grad_norm": 14.286989212036133,
      "learning_rate": 4.647047779968084e-05,
      "loss": 3.1648,
      "step": 1505
    },
    {
      "epoch": 0.2126161644607153,
      "grad_norm": 7.168178081512451,
      "learning_rate": 4.6458744015770205e-05,
      "loss": 3.0294,
      "step": 1510
    },
    {
      "epoch": 0.21332019149535342,
      "grad_norm": 17.789897918701172,
      "learning_rate": 4.6447010231859574e-05,
      "loss": 2.5579,
      "step": 1515
    },
    {
      "epoch": 0.21402421852999154,
      "grad_norm": 14.39465618133545,
      "learning_rate": 4.6435276447948936e-05,
      "loss": 2.7271,
      "step": 1520
    },
    {
      "epoch": 0.2147282455646297,
      "grad_norm": 12.277078628540039,
      "learning_rate": 4.64235426640383e-05,
      "loss": 3.1659,
      "step": 1525
    },
    {
      "epoch": 0.2154322725992678,
      "grad_norm": 14.313053131103516,
      "learning_rate": 4.6411808880127667e-05,
      "loss": 2.9899,
      "step": 1530
    },
    {
      "epoch": 0.21613629963390593,
      "grad_norm": 10.179645538330078,
      "learning_rate": 4.6400075096217035e-05,
      "loss": 2.4889,
      "step": 1535
    },
    {
      "epoch": 0.21684032666854408,
      "grad_norm": 10.151788711547852,
      "learning_rate": 4.638834131230639e-05,
      "loss": 3.2301,
      "step": 1540
    },
    {
      "epoch": 0.2175443537031822,
      "grad_norm": 13.998218536376953,
      "learning_rate": 4.637660752839576e-05,
      "loss": 2.7833,
      "step": 1545
    },
    {
      "epoch": 0.21824838073782032,
      "grad_norm": 17.469520568847656,
      "learning_rate": 4.636487374448512e-05,
      "loss": 3.1414,
      "step": 1550
    },
    {
      "epoch": 0.21895240777245847,
      "grad_norm": 7.275015354156494,
      "learning_rate": 4.635313996057449e-05,
      "loss": 2.9356,
      "step": 1555
    },
    {
      "epoch": 0.2196564348070966,
      "grad_norm": 9.53085708618164,
      "learning_rate": 4.634140617666385e-05,
      "loss": 2.6886,
      "step": 1560
    },
    {
      "epoch": 0.22036046184173472,
      "grad_norm": 15.691338539123535,
      "learning_rate": 4.6329672392753214e-05,
      "loss": 2.833,
      "step": 1565
    },
    {
      "epoch": 0.22106448887637284,
      "grad_norm": 21.03827667236328,
      "learning_rate": 4.631793860884258e-05,
      "loss": 3.03,
      "step": 1570
    },
    {
      "epoch": 0.221768515911011,
      "grad_norm": 12.751347541809082,
      "learning_rate": 4.6306204824931945e-05,
      "loss": 2.9105,
      "step": 1575
    },
    {
      "epoch": 0.2224725429456491,
      "grad_norm": 8.989747047424316,
      "learning_rate": 4.629447104102131e-05,
      "loss": 3.4873,
      "step": 1580
    },
    {
      "epoch": 0.22317656998028723,
      "grad_norm": 15.320913314819336,
      "learning_rate": 4.6282737257110676e-05,
      "loss": 3.8062,
      "step": 1585
    },
    {
      "epoch": 0.22388059701492538,
      "grad_norm": 15.389135360717773,
      "learning_rate": 4.6271003473200044e-05,
      "loss": 2.7226,
      "step": 1590
    },
    {
      "epoch": 0.2245846240495635,
      "grad_norm": 15.116156578063965,
      "learning_rate": 4.62592696892894e-05,
      "loss": 3.2454,
      "step": 1595
    },
    {
      "epoch": 0.22528865108420162,
      "grad_norm": 8.350181579589844,
      "learning_rate": 4.624753590537877e-05,
      "loss": 3.9066,
      "step": 1600
    },
    {
      "epoch": 0.22599267811883977,
      "grad_norm": 23.547300338745117,
      "learning_rate": 4.623580212146814e-05,
      "loss": 3.2659,
      "step": 1605
    },
    {
      "epoch": 0.2266967051534779,
      "grad_norm": 21.18507957458496,
      "learning_rate": 4.62240683375575e-05,
      "loss": 2.6424,
      "step": 1610
    },
    {
      "epoch": 0.22740073218811602,
      "grad_norm": 11.96772575378418,
      "learning_rate": 4.621233455364686e-05,
      "loss": 3.1517,
      "step": 1615
    },
    {
      "epoch": 0.22810475922275417,
      "grad_norm": 13.403164863586426,
      "learning_rate": 4.620060076973622e-05,
      "loss": 2.9434,
      "step": 1620
    },
    {
      "epoch": 0.2288087862573923,
      "grad_norm": 9.053540229797363,
      "learning_rate": 4.618886698582559e-05,
      "loss": 2.6842,
      "step": 1625
    },
    {
      "epoch": 0.2295128132920304,
      "grad_norm": 24.579038619995117,
      "learning_rate": 4.6177133201914954e-05,
      "loss": 2.8919,
      "step": 1630
    },
    {
      "epoch": 0.23021684032666853,
      "grad_norm": 7.319680213928223,
      "learning_rate": 4.6165399418004316e-05,
      "loss": 3.1023,
      "step": 1635
    },
    {
      "epoch": 0.23092086736130668,
      "grad_norm": 9.243276596069336,
      "learning_rate": 4.6153665634093685e-05,
      "loss": 4.2022,
      "step": 1640
    },
    {
      "epoch": 0.2316248943959448,
      "grad_norm": 13.613616943359375,
      "learning_rate": 4.6141931850183054e-05,
      "loss": 2.2249,
      "step": 1645
    },
    {
      "epoch": 0.23232892143058292,
      "grad_norm": 17.682710647583008,
      "learning_rate": 4.613019806627241e-05,
      "loss": 2.41,
      "step": 1650
    },
    {
      "epoch": 0.23303294846522107,
      "grad_norm": 9.793351173400879,
      "learning_rate": 4.611846428236178e-05,
      "loss": 2.5944,
      "step": 1655
    },
    {
      "epoch": 0.2337369754998592,
      "grad_norm": 8.758495330810547,
      "learning_rate": 4.6106730498451146e-05,
      "loss": 3.035,
      "step": 1660
    },
    {
      "epoch": 0.23444100253449732,
      "grad_norm": 15.683401107788086,
      "learning_rate": 4.609499671454051e-05,
      "loss": 2.6234,
      "step": 1665
    },
    {
      "epoch": 0.23514502956913547,
      "grad_norm": 5.078031063079834,
      "learning_rate": 4.608326293062987e-05,
      "loss": 2.6522,
      "step": 1670
    },
    {
      "epoch": 0.2358490566037736,
      "grad_norm": 10.280447959899902,
      "learning_rate": 4.607152914671924e-05,
      "loss": 3.4488,
      "step": 1675
    },
    {
      "epoch": 0.2365530836384117,
      "grad_norm": 6.4069952964782715,
      "learning_rate": 4.60597953628086e-05,
      "loss": 2.9809,
      "step": 1680
    },
    {
      "epoch": 0.23725711067304983,
      "grad_norm": 12.590914726257324,
      "learning_rate": 4.604806157889796e-05,
      "loss": 3.9466,
      "step": 1685
    },
    {
      "epoch": 0.23796113770768798,
      "grad_norm": 9.200961112976074,
      "learning_rate": 4.6036327794987325e-05,
      "loss": 3.8981,
      "step": 1690
    },
    {
      "epoch": 0.2386651647423261,
      "grad_norm": 10.87151050567627,
      "learning_rate": 4.6024594011076694e-05,
      "loss": 2.5423,
      "step": 1695
    },
    {
      "epoch": 0.23936919177696422,
      "grad_norm": 9.614073753356934,
      "learning_rate": 4.601286022716606e-05,
      "loss": 2.6654,
      "step": 1700
    },
    {
      "epoch": 0.24007321881160237,
      "grad_norm": 5.590569972991943,
      "learning_rate": 4.600112644325542e-05,
      "loss": 2.5686,
      "step": 1705
    },
    {
      "epoch": 0.2407772458462405,
      "grad_norm": 9.3706693649292,
      "learning_rate": 4.598939265934479e-05,
      "loss": 3.115,
      "step": 1710
    },
    {
      "epoch": 0.24148127288087862,
      "grad_norm": 30.15271759033203,
      "learning_rate": 4.5977658875434155e-05,
      "loss": 2.3956,
      "step": 1715
    },
    {
      "epoch": 0.24218529991551677,
      "grad_norm": 18.624759674072266,
      "learning_rate": 4.596592509152352e-05,
      "loss": 2.6474,
      "step": 1720
    },
    {
      "epoch": 0.2428893269501549,
      "grad_norm": 10.862142562866211,
      "learning_rate": 4.595419130761288e-05,
      "loss": 2.7661,
      "step": 1725
    },
    {
      "epoch": 0.243593353984793,
      "grad_norm": 14.901156425476074,
      "learning_rate": 4.594245752370225e-05,
      "loss": 2.9915,
      "step": 1730
    },
    {
      "epoch": 0.24429738101943116,
      "grad_norm": 9.732704162597656,
      "learning_rate": 4.593072373979161e-05,
      "loss": 2.8115,
      "step": 1735
    },
    {
      "epoch": 0.24500140805406928,
      "grad_norm": 14.108150482177734,
      "learning_rate": 4.591898995588097e-05,
      "loss": 3.0919,
      "step": 1740
    },
    {
      "epoch": 0.2457054350887074,
      "grad_norm": 14.980766296386719,
      "learning_rate": 4.5907256171970334e-05,
      "loss": 2.4614,
      "step": 1745
    },
    {
      "epoch": 0.24640946212334552,
      "grad_norm": 9.464054107666016,
      "learning_rate": 4.58955223880597e-05,
      "loss": 2.8698,
      "step": 1750
    }
  ],
  "logging_steps": 5,
  "max_steps": 21306,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 28662511104000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
